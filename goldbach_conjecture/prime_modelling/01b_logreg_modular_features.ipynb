{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637a57b2-cbd6-4384-8bac-a66f0e91ff13",
   "metadata": {},
   "source": [
    "### Prove of concept: we should be able to build an almost perfect prime model on modular features \n",
    "**modular feature**: is a number dividable by a given prime <br>\n",
    "target whether int is/ isnt prime is almost a simple linear combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf84b8-2f74-47b4-9c1a-ca2a8aae67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# custom imports\n",
    "from transform_fcts import (\n",
    "    calculate_modular_features,\n",
    "    binary_features,\n",
    "    prime_distribution_features,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274287b6-531b-4438-a5d1-d6ff79f470d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "prime_lim = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3247c89-0cd7-4faa-a74d-763fd7201bdf",
   "metadata": {},
   "source": [
    "#### Build Core Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc926c9-baa4-4ce6-ad32-497a1b6b11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in prime numbers\n",
    "primes = np.load(f'../../artifacts/primes/prime_{prime_lim}.npy')\n",
    "primes[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311335e-df2c-453d-93d4-8ee2cd6a97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to natural numbers with binary target\n",
    "natural_numbers = np.arange(0,prime_lim)\n",
    "target = np.zeros(prime_lim, dtype=bool)\n",
    "target[primes] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ea8ec-5517-42c6-a9fa-1c0734978ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data={'n': natural_numbers[2:], 'y': target[2:]})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30ace8-8152-4e3c-b249-4c51b4894438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### A: try out small data sets with modular features\n",
    "--> if we can actually almost garantie a modular signal per prime in training, the models should show almost perfect performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843951f1-8247-4b84-b054-d5baab47b10b",
   "metadata": {},
   "source": [
    "models: \n",
    "- prime cutoff 10000, and the lower 100 primes are used for features --> converges\n",
    "- prime cutoff 100000, and the lower 100 primes are used for features --> converges\n",
    "- prime cutoff 500000, and the lower 100 primes are used for features --> converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7cd76-5770-4d3f-8fc4-0cf989e70ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    10000: {},\n",
    "    100000: {},\n",
    "    prime_lim: {},\n",
    "} # prime_cutoff as key for models\n",
    "\n",
    "n_modular_features = 100 # not all features\n",
    "target_col = 'y'\n",
    "\n",
    "\n",
    "for prime_cutoff in model_dict.keys():\n",
    "    print(prime_cutoff,'\\n')\n",
    "    \n",
    "    data_a = data[data['n']<prime_cutoff].copy()\n",
    "    print(data_a.shape)\n",
    "    \n",
    "    # create modular features\n",
    "    features = [data_a['n'].apply(lambda x: 1 if (x%prime==0 and x!=prime) else 0).values for prime in primes[:n_modular_features]]\n",
    "    features = np.array(features).T\n",
    "    feature_col = [f\"mod_{str(prime)}\" for prime in primes[:n_modular_features]]\n",
    "\n",
    "    data_a = pd.concat([data_a, pd.DataFrame(features, columns=feature_col)], axis=1)\n",
    "\n",
    "    print(data_a.head())\n",
    "\n",
    "    # split in train & test    \n",
    "    X, y = data_a[feature_col], data_a[target_col]\n",
    "    print(target_col in feature_col)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # train logistic regression as start\n",
    "    # lbfgs solver, l2 penalty\n",
    "    clf = LogisticRegressionCV(cv=10, random_state=0, max_iter=500).fit(X_train, y_train)\n",
    "    \n",
    "    # store models and data\n",
    "    model_dict[prime_cutoff]['data'] = data_a.copy()\n",
    "    model_dict[prime_cutoff]['model'] = clf\n",
    "    \n",
    "    model_dict[prime_cutoff]['X_train'] = X_train.copy()\n",
    "    model_dict[prime_cutoff]['X_test'] = X_test.copy()\n",
    "    model_dict[prime_cutoff]['y_train'] = y_train.copy()\n",
    "    model_dict[prime_cutoff]['y_test'] = y_test.copy()\n",
    "\n",
    "    print('Training completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212576c-c446-410e-8678-c8b8c894a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for evaluation of models\n",
    "\n",
    "for prime_cutoff in model_dict.keys():\n",
    "    curmod = model_dict[prime_cutoff]\n",
    "    curmod['y_pred'] = curmod['model'].predict(curmod['X_test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe6a8a-8926-4c00-839e-51158d3f9030",
   "metadata": {},
   "source": [
    "#### Check overall performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b5ed1-6919-4b3a-ac73-f233df5b6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prime_cutoff in model_dict.keys():\n",
    "    print(f'model with prime cutoff {prime_cutoff}')\n",
    "    curmod = model_dict[prime_cutoff]\n",
    "    print('confusion matrix \\n', confusion_matrix(curmod['y_test'], curmod['y_pred']), '\\n')\n",
    "\n",
    "    print(classification_report(curmod['y_test'], curmod['y_pred']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2642a-bd25-4f1c-840e-f4fb5972b56e",
   "metadata": {},
   "source": [
    "very few misclassification in every model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd40bfd-27ee-4ce3-b5a6-5b892f1f2beb",
   "metadata": {},
   "source": [
    "#### Confusion matrix depending on signal in modular features for models\n",
    "**When do we have misclassifications?** <br>\n",
    "-> all false positives (not prime, but predicted as prime) should have no signal in modular features (like no single modular feature = 1) <br>\n",
    "-> all false negatives (prime, but not predicted as prime) cannot have any modular signal (as they are not prime) -> so how does this misclassification happen? <br>\n",
    "\n",
    "<br> \n",
    "- ideally, the model would perfectly learn that no modular signal = prime -> that would eliminate all false negatives <br>\n",
    "- introducing other features than just modular features would help to reduce the false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f066a-effc-4949-89fe-b3f45d22c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prime_cutoff in enumerate(model_dict.keys()):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(9, 3.5))\n",
    "\n",
    "    curmod = model_dict[prime_cutoff]\n",
    "\n",
    "    mod_features = curmod['model'].feature_names_in_\n",
    "\n",
    "    # add new superposition of modular features to dataframe\n",
    "    curmod['X_test']['any_mod'] = curmod['X_test'][mod_features].aggregate('sum',axis=1)>0\n",
    "\n",
    "    # confusion matrix with any modular features\n",
    "    cm_mod = confusion_matrix(curmod['y_test'][curmod['X_test']['any_mod']], curmod['y_pred'][curmod['X_test']['any_mod']], labels=curmod['model'].classes_)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mod,\n",
    "                              display_labels=curmod['model'].classes_)\n",
    "\n",
    "    disp.plot(ax=ax[0])\n",
    "    \n",
    "    # confusion matrix without any modular features\n",
    "    cm_nonmod = confusion_matrix(curmod['y_test'][curmod['X_test']['any_mod']==False], curmod['y_pred'][curmod['X_test']['any_mod']==False], labels=curmod['model'].classes_)\n",
    "\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_nonmod,\n",
    "                                  display_labels=curmod['model'].classes_)\n",
    "    disp.plot(ax=ax[1])\n",
    "\n",
    "    ax[0].set_title('Modular features = 1', size=10)\n",
    "    ax[1].set_title('Modular features = 0', size=10)\n",
    "\n",
    "    plt.suptitle(f\"Model with prime cutoff {prime_cutoff}\")\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024c3fd-234a-4926-942a-470f516db0ae",
   "metadata": {},
   "source": [
    "- first model has perfect classification\n",
    "- second model classifies some primes with instead of modular features, but no modular feature = prime (and is true in test set)\n",
    "- third model correctly classifies any number with any modular signal as \"not prime\" but misclassifies all which are prime although there is no modular signal\n",
    "  --> we now have to find other features which might help with this false positive group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0cdb8-aec9-46e3-b2a9-3bc2d10a8ea5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### B: lets try to move away from modular features\n",
    "- modular features are trivial, because if we provide them all, the recognition of \"prime / no prime\" is a simple linear superposition <br>\n",
    "- lets try to find other features and reduce modular features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8b8ca-f960-4682-9995-ee3cbe07a3aa",
   "metadata": {},
   "source": [
    "**models:**\n",
    "- prime cutoff 500000, and the lower 50 primes are used for features, no other features -> converges\n",
    "- prime cutoff 500000, and the lower 50 primes are used for features, some other normalized features added\n",
    "  -> converges, other features seem completely unimportant\n",
    "- prime cutoff 500000, and the lower 10 primes are used for features and modular features for n+1 and n-1 are introduced, some other normalized features added\n",
    "  -> converges, first model with kinda interesting mixture of true & false classification\n",
    "  -> we reaching the area where the modular features of n are not enough to classify the prime, and the model has to rely on its other featuers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2cd73a-574c-48bc-9930-627db3b1e28f",
   "metadata": {},
   "source": [
    "**how can we characterize the space of numbers which helps to find whether a number is a prime**\n",
    "\n",
    "- distance to last prime: kinda gives info about the prime density (cannot be 0)\n",
    "- distance between the previous 2 prime numbers? (thats prob just introduce noise)\n",
    "- number of primes lower n: that feature will be repetitive for a lot of numbers\n",
    "- we could have modular info about the neighbours of n: n+1, n-1 (eeh that might also just introduce noise)\n",
    "- lets try that: modular info about the first ten primes, for n, n+1 and n-1\n",
    "\n",
    "-> look afterwards at feature importance, like did anything have a minimal chance compared to modular info for n <br>\n",
    "-> compare performance to modular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c9d64-8186-4ef7-a8a2-6455b36a7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modular_features = 10 # not all features\n",
    "target_col = 'y'\n",
    "\n",
    "data_b = data[data['n']<prime_lim].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a50d3f-c3cb-4c90-9028-1e052d9912e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create modular features\n",
    "\n",
    "data_b = calculate_modular_features(data_b, n_distance=0, primes=primes, n_modular_features=n_modular_features)\n",
    "\n",
    "# print(data_b.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f363007-65da-4f8a-a720-2f80809164fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b = calculate_modular_features(data_b, n_distance=1)\n",
    "data_b = calculate_modular_features(data_b, n_distance=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec94137-1a1d-4225-b5af-9b6014178cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a633251-1ed5-4f35-b182-f05c1eef0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut off last row of data_b as n+1_mod_features are not defined for last row\n",
    "data_b = data_b.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac61a34-0d39-4017-badc-009ae2b29c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for further calculation\n",
    "data_b['last_prime']=data_b['n'].apply(lambda x: primes[primes<x].max() if x!=2 else -1)\n",
    "data_b['2nd_last_prime']=data_b['last_prime'].apply(lambda x: primes[primes<x].max() if x>2 else -1)\n",
    "\n",
    "# non-modular features\n",
    "data_b['primes_lower_n']=data_b['n'].apply(lambda x: len(primes[primes<x]) if x!=2 else 0)\n",
    "data_b['distance_to_last_prime']=data_b.apply(lambda x: x['n']-x['last_prime'] if x['n']!=2 else -1, axis=1)\n",
    "data_b['distance_between_last2primes']=data_b.apply(lambda x: x['last_prime']-x['2nd_last_prime'] if x['n']>3 else -1, axis=1)\n",
    "\n",
    "#print(data_b[['n','last_prime', '2nd_last_prime','primes_lower_n','distance_to_last_prime', 'distance_between_last2primes']].head(20))\n",
    "\n",
    "data_b.drop(columns=['last_prime','2nd_last_prime'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3ed8d-abeb-47ed-911f-0f127853d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = data_b.columns.drop(target_col)\n",
    "print(target_col in feature_col)\n",
    "print(len(feature_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cec54-75b0-4b1d-80ad-06baa84c3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_b[feature_col] = scaler.fit_transform(data_b[feature_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b72292-edf7-4d4f-a3ca-4b1e0c7933bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train & test    \n",
    "X, y = data_b[feature_col], data_b[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train logistic regression as start\n",
    "# lbfgs solver, l2 penalty\n",
    "clf = LogisticRegressionCV(cv=10, random_state=0, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c5383-c5a3-49e2-8de3-f82a9d15e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e423047-97c1-4134-a81a-2ee329223fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedcb3a-1e6d-4c6a-89c3-7c95d581ad04",
   "metadata": {},
   "source": [
    "#### Feature importance inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da12c0-b40a-4235-8dc1-36fa58ab0e17",
   "metadata": {},
   "source": [
    "Feature importance due to coefficients of regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b443a-dc08-4596-9f7c-79767c54bcbd",
   "metadata": {},
   "source": [
    "the coeff plot kinda indicates that the model has no idea what its doing tbh... <br> \n",
    "I expected the n_mod-features to outrank everything by far. surprised that the n+1-mod / n-1-mod features can keep up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c4305-0041-4206-8f99-0574a9cd84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefs != feature importance...\n",
    "feature_importance = pd.DataFrame(data={'features': clf.feature_names_in_, 'coeffs': np.abs(clf.coef_[0])})\n",
    "coeff_importance = feature_importance.sort_values('coeffs', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3dc80-bbf3-40de-b1e9-f9ca955ef965",
   "metadata": {},
   "source": [
    "Feature importance from mutual info (entropy of target expression in relation to a features different values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7a0a1-d6cf-42ae-989a-9768c112e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# mutual info completely independent from model\n",
    "mutual_info_out = mutual_info_classif(X_test, y_test, random_state=0)\n",
    "feature_importance['mutual_info'] = mutual_info_out\n",
    "mutual_info_importance = feature_importance.sort_values('mutual_info', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7009aa-be5c-473a-a38f-466454dea31b",
   "metadata": {},
   "source": [
    "Permutation feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccffaa-6704-43ec-893a-ad0c7202eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "permut_out = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=0)\n",
    "feature_importance['permutation_importance'] = permut_out['importances_mean']\n",
    "permut_importance = feature_importance.sort_values('permutation_importance', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2800d7-7373-4809-9b85-4ed9a7fb002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(20,10))\n",
    "\n",
    "coeff_importance.plot(x='features', y='coeffs', kind='barh', ax=ax[0], title='Regression coeffs')\n",
    "mutual_info_importance.plot(x='features', y='mutual_info', kind='barh', ax=ax[1], title='Mutual info')\n",
    "permut_importance.plot(x='features', y='permutation_importance', kind='barh', ax=ax[2], title='Permutation importance')\n",
    "\n",
    "ax[1].yaxis.label.set_visible(False)\n",
    "ax[2].yaxis.label.set_visible(False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404a696-f60d-4bad-a881-bb0510d4596f",
   "metadata": {},
   "source": [
    "- Coeffs are very binary in their assessment of whats important and what is not -> all modular features are important, the rest is not\n",
    "- Mutual Info vs Permutation actually give a nice glimpse into the model quality: the fact that we have a nice correlation here shows that the model is not completely hallucinating\n",
    "- it kinda fails to give meaning to the distance_last_primes though -> has meaning for target, but fails to have impact on prediction in permutation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d9a69-fac4-4ac0-8095-37c628fb7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(12, 3))\n",
    "\n",
    "feature_importance.plot(x='mutual_info', y='coeffs', kind='scatter', ax=ax[0], title='Mutual Info vs Coeffs')\n",
    "feature_importance.plot(x='permutation_importance', y='coeffs', kind='scatter', ax=ax[1], title='Permutation vs Coeffs')\n",
    "feature_importance.plot(x='mutual_info', y='permutation_importance', kind='scatter', ax=ax[2], title='Mutual Info vs Permutation')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d1310-b515-4a82-9215-bb66a7988366",
   "metadata": {},
   "source": [
    "### C: Only using modular features of n-neighbours (n+1, n-1, etc) and non-modular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae8c48-b51e-4634-bb00-2137e0f57935",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modular_features = 20 # not all features\n",
    "target_col = 'y'\n",
    "\n",
    "data_c = data[data['n']<prime_lim].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed167182-756d-4195-a9ea-77f7bab2d57b",
   "metadata": {},
   "source": [
    "Modular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e581e35-3d53-4b8b-96b4-3b1f4e008ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create modular features\n",
    "\n",
    "mod_feature_df = calculate_modular_features(data_c, 0, primes=primes, n_modular_features=n_modular_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65cdd1-5e15-40b1-8864-2664bb366228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modular features of neighbours of n\n",
    "\n",
    "n_neighbours = 2\n",
    "\n",
    "for n_dist in range(1, n_neighbours+1):\n",
    "    data_c = calculate_modular_features(data_df=data_c, n_distance=n_dist, mod_df=mod_feature_df)\n",
    "    data_c = calculate_modular_features(data_df=data_c, n_distance=-n_dist, mod_df=mod_feature_df)\n",
    "\n",
    "data_c = data_c.iloc[:-n_neighbours]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe870afb-efc6-4922-8614-7afcc5199005",
   "metadata": {},
   "source": [
    "Non-modular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398081f-1374-4b79-9a74-74a33cff18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of binary representation\n",
    "data_c = binary_features(data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eda012-d731-474a-8f60-89d06ae6742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prime distribution features\n",
    "data_c = prime_distribution_features(data_c, primes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f76e8b-a969-414c-8c57-5e7a9137f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01909c0-780d-43d4-afd8-2de8c1bcf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = data_c.columns.drop(target_col)\n",
    "print(target_col in feature_col)\n",
    "print(len(feature_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8968f7-87d2-482b-8475-4732f91599de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_c[feature_col] = scaler.fit_transform(data_c[feature_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350a3d7-2733-4b58-be64-540fc2bc0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train & test    \n",
    "X, y = data_c[feature_col], data_c[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train logistic regression as start\n",
    "# lbfgs solver, l2 penalty\n",
    "clf = LogisticRegressionCV(cv=10, random_state=0, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb640c02-f779-4767-abf1-44ddfb80a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def binarize_prediction(y_test, y_pred):\n",
    "    \n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)  # Test thresholds between 0 and 1\n",
    "    \n",
    "    best_f1 = 0\n",
    "    # Iterate through different thresholds to find the best one\n",
    "    for threshold in thresholds:\n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred_bin = (y_pred >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_test, y_pred_bin)\n",
    "            \n",
    "        # Check if this threshold gives a better F1 score\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return (y_pred >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69e058-27cf-4f06-a59c-5e2f2a6881f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_proba = clf.predict_proba(X_test)\n",
    "#y_pred = binarize_prediction(y_test, y_pred)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c26540-541e-43b0-bc7f-60434a8ba2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fa404-4ec8-4909-8058-2422075d18b8",
   "metadata": {},
   "source": [
    "--> model decides to just perform bad on the positive class -> you see the effect of high class-imbalance here <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d56da9-c3bd-4d34-84aa-4a42edd0bbfb",
   "metadata": {},
   "source": [
    "#### Feature importance inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fb7b4-4ec2-40c6-a794-e8db57e53a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# mutual info completely independent from model\n",
    "mutual_info_out = mutual_info_classif(X_test, y_test, random_state=0)\n",
    "\n",
    "feature_importance = pd.DataFrame(data={'features': clf.feature_names_in_, 'mutual_info': mutual_info_out})\n",
    "\n",
    "mutual_info_importance = feature_importance.sort_values('mutual_info', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d9fad-b58e-42c1-b5e8-3eb7ecf52ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "permut_out = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=0)\n",
    "feature_importance['permutation_importance'] = permut_out['importances_mean']\n",
    "permut_importance = feature_importance.sort_values('permutation_importance', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5074ab4-6ff9-4823-a57d-8f221e2b5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(13,15))\n",
    "\n",
    "mutual_info_importance.plot(x='features', y='mutual_info', kind='barh', ax=ax[0], title='Mutual info')\n",
    "permut_importance.plot(x='features', y='permutation_importance', kind='barh', ax=ax[1], title='Permutation importance')\n",
    "\n",
    "ax[0].yaxis.label.set_visible(False)\n",
    "ax[1].yaxis.label.set_visible(False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eefe96-eaa3-4bfd-a415-63caba186860",
   "metadata": {},
   "source": [
    "--> not really happy with that. binary features have a high mutual info, but dont impact the model in the permutation performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
